{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-STGAC model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbD2a-mNGlFn",
        "outputId": "c56c3bb3-51eb-481a-c2e8-977dc38a656e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the necessary python packages.**"
      ],
      "metadata": {
        "id": "-EzwQwxes4Ai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JdJzRRICRRPM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as utils\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "holHEJXJ8Oqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Data**.\n",
        "\n",
        "\n",
        "\n",
        "``` Main matrix ``` with 410 stops, starting from ```2021-09-13 05:00:00``` to ```2021-10-10 23:00:00```.\n",
        "\n",
        "```A``` is the corresponding adjacency matrix, and ```BNP``` is the corresponding  matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "WFq5k3N67aAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_df = pd.read_pickle(\"/content/drive/MyDrive/pfe/main_matrix_5_23.pkl\")\n",
        "with open('/content/drive/MyDrive/pfe/A_5_23','rb') as f:\n",
        "     A = pickle.load(f)\n",
        "BNP = np.load('FFR.npy')"
      ],
      "metadata": {
        "id": "uRePNzjpYNqT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preparation**\n",
        "\n",
        "```ConcatDataset``` class help to consider 3 types of datasets simulatnously (recent, daily-periodic and weekly-periodic data)."
      ],
      "metadata": {
        "id": "zOh9Kip4til5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[i %len(d)] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(d) for d in self.datasets)"
      ],
      "metadata": {
        "id": "zwm0nuCGpsIz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PrepareDataset(main_passenger_matrix, BATCH_SIZE = 20,  pred_len = 1, train_propotion = 0.7, valid_propotion = 0.2):\n",
        "\n",
        "    time_len = main_passenger_matrix.shape[0]\n",
        "    \n",
        "    max_load = main_passenger_matrix.max().max()\n",
        "    main_passenger_matrix =  main_passenger_matrix / max_load\n",
        "    \n",
        "\n",
        "    # Weekly-periodic data preparation\n",
        "    pass_sequences, pass_labels = [], []\n",
        "    \n",
        "    # Number of historical sequence\n",
        "    seq_len = 2\n",
        "\n",
        "    for i in range(time_len - 109*7*(seq_len + pred_len)):\n",
        "        pass_sequences.append(main_passenger_matrix.iloc[[i+109*7*j for j in range(seq_len)]].values)  #109 is the number of steps between two consecutive days\n",
        "        pass_labels.append(main_passenger_matrix.iloc[[i+109*7*(seq_len+j) for j in range(pred_len)]].values)\n",
        "        \n",
        "        \n",
        "\n",
        "    pass_sequences, pass_labels = np.asarray(pass_sequences), np.asarray(pass_labels)\n",
        "    \n",
        "    sample_size = pass_sequences.shape[0]\n",
        "    \n",
        "    \n",
        "    train_index = int(np.floor(sample_size * train_propotion))  \n",
        "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
        "    \n",
        "    # randomize the order of sequences\n",
        "    \n",
        "    c = list(zip(pass_sequences, pass_labels))\n",
        "    random.shuffle(c)\n",
        "    pass_sequences, pass_labels = zip(*c)\n",
        "   \n",
        "    train_data, train_label = pass_sequences[:train_index], pass_labels[:train_index]    \n",
        "    valid_data, valid_label = pass_sequences[train_index:valid_index], pass_labels[train_index:valid_index] \n",
        "    test_data, test_label = pass_sequences[valid_index:], pass_labels[valid_index:]\n",
        "    \n",
        "    \n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label) \n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label) \n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "    \n",
        "   \n",
        "    train_dataset_w = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset_w = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset_w = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "\n",
        "    # Daily-periodic data preparation\n",
        "\n",
        "    seq_len = 4\n",
        "    pass_sequences, pass_labels = [], []\n",
        "    \n",
        "    # Specify the time range from which to get the data\n",
        "    maind = main_passenger_matrix.loc['2021-09-23 05:00:00':'2021-10-04 23:00:00']\n",
        "\n",
        "    time_len = maind.shape[0]\n",
        "\n",
        "    for i in range(time_len - 109*(seq_len + pred_len)):\n",
        "        pass_sequences.append(maind.iloc[[i+109*j for j in range(seq_len)]].values)\n",
        "        pass_labels.append(maind.iloc[[i+109*(seq_len+j) for j in range(pred_len)]].values)\n",
        "\n",
        "        \n",
        " \n",
        "    pass_sequences, pass_labels = np.asarray(pass_sequences), np.asarray(pass_labels)\n",
        "    \n",
        "    sample_size = pass_sequences.shape[0]   \n",
        "    index = np.arange(sample_size, dtype = int) \n",
        "    np.random.shuffle(index)\n",
        "    \n",
        "    train_index = int(np.floor(sample_size * train_propotion))  \n",
        "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
        "    \n",
        "    # randomize the order of sequences\n",
        "    \n",
        "    c = list(zip(pass_sequences, pass_labels))\n",
        "    random.shuffle(c)\n",
        "    pass_sequences, pass_labels = zip(*c)\n",
        "   \n",
        "    train_data, train_label = pass_sequences[:train_index], pass_labels[:train_index]    \n",
        "    valid_data, valid_label = pass_sequences[train_index:valid_index], pass_labels[train_index:valid_index] \n",
        "    test_data, test_label = pass_sequences[valid_index:], pass_labels[valid_index:]\n",
        "    \n",
        "    \n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label) \n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label) \n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "    \n",
        "   \n",
        "    train_dataset_d = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset_d = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset_d = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "    # Recent data preparation\n",
        "\n",
        "    seq_len = 10\n",
        "    pass_sequences, pass_labels = [], []\n",
        "    \n",
        "    # Specify the time range from which to get the data\n",
        "    mainr = main_passenger_matrix.loc['2021-09-26 21:30:00':'2021-10-04 05:00:00']\n",
        "\n",
        "    time_len = mainr.shape[0]\n",
        "    for i in range(time_len - (seq_len + pred_len)):\n",
        "        pass_sequences.append(mainr.iloc[[i+ j for j in range(seq_len)]].values)  \n",
        "        pass_labels.append(mainr.iloc[[i+(seq_len+j) for j in range(pred_len)]].values)\n",
        "       \n",
        "    \n",
        "    pass_sequences, pass_labels = np.asarray(pass_sequences), np.asarray(pass_labels)\n",
        "    \n",
        "    sample_size = pass_sequences.shape[0]   \n",
        "    index = np.arange(sample_size, dtype = int) \n",
        "    np.random.shuffle(index) \n",
        "    \n",
        "    train_index = int(np.floor(sample_size * train_propotion))  \n",
        "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
        "    \n",
        "    # randomize the order of sequences\n",
        "    \n",
        "    c = list(zip(pass_sequences, pass_labels))\n",
        "    random.shuffle(c)\n",
        "    pass_sequences, pass_labels = zip(*c)\n",
        "    \n",
        "       \n",
        "    train_data, train_label = pass_sequences[:train_index], pass_labels[:train_index]    \n",
        "    valid_data, valid_label = pass_sequences[train_index:valid_index], pass_labels[train_index:valid_index] \n",
        "    test_data, test_label = pass_sequences[valid_index:], pass_labels[valid_index:]    \n",
        "    \n",
        "    \n",
        "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label) \n",
        "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label) \n",
        "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
        "    \n",
        "   \n",
        "    train_dataset_r = utils.TensorDataset(train_data, train_label)\n",
        "    valid_dataset_r = utils.TensorDataset(valid_data, valid_label)\n",
        "    test_dataset_r = utils.TensorDataset(test_data, test_label)\n",
        "\n",
        "\n",
        "\n",
        "    # All the data in one dataloader\n",
        "    \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "                ConcatDataset(train_dataset_r, train_dataset_d,train_dataset_w),\n",
        "                batch_size=BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "    \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "                ConcatDataset(valid_dataset_r, valid_dataset_d,valid_dataset_w),\n",
        "                batch_size=BATCH_SIZE, shuffle=True, drop_last =True)\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "                ConcatDataset(test_dataset_r, test_dataset_d,test_dataset_w),\n",
        "                batch_size=BATCH_SIZE, shuffle=True, drop_last= True)\n",
        "    \n",
        "    return train_loader, valid_loader, test_loader, max_load"
      ],
      "metadata": {
        "id": "a7iz6r_2YxqA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "OnzmmFpilROQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporal Gated Dilated Convolution (Temporal-GDCN) Layer "
      ],
      "metadata": {
        "id": "jpVllU2ybefJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GDCN(nn.Module):\n",
        "\n",
        "  def __init__(self, in_dim=1, blocks=2, layers = 2 , residual_channels=1, dilation_channels=1, kernel_size=2):\n",
        "    super(GDCN, self).__init__()\n",
        "    self.blocks = blocks\n",
        "    self.layers = layers\n",
        "    self.filter_convs = nn.ModuleList()\n",
        "    self.gate_convs = nn.ModuleList()\n",
        "    self.start_conv = nn.Conv2d(in_channels=in_dim, out_channels=residual_channels, kernel_size=(1,1), padding='same')\n",
        "  \n",
        "    for b in range(blocks):\n",
        "      new_dilation = 1\n",
        "      for i in range(layers):\n",
        "        #dialated convolutions\n",
        "        self.filter_convs.append(nn.Conv2d(in_channels=residual_channels, out_channels=dilation_channels, \n",
        "                                           kernel_size=(1,kernel_size), dilation=new_dilation, padding='same'))\n",
        "        self.gate_convs.append(nn.Conv2d(in_channels=residual_channels, out_channels=dilation_channels,\n",
        "                                         kernel_size=(1, kernel_size), dilation=new_dilation, padding='same'))\n",
        "\n",
        "  def forward(self, input):\n",
        "      \n",
        "    x = torch.unsqueeze(torch.unsqueeze(input,0),0)\n",
        "    x = self.start_conv(x)\n",
        "\n",
        "    for i in range(self.blocks*self.layers):\n",
        "      residual = x\n",
        "\n",
        "      # dilated convolution\n",
        "      filter = self.filter_convs[i](residual)\n",
        "      filter = torch.tanh(filter)\n",
        "           \n",
        "      gate = self.gate_convs[i](residual)\n",
        "      gate = torch.sigmoid(gate)\n",
        "\n",
        "      x = filter * gate\n",
        "\n",
        "    x = torch.squeeze(x,1)\n",
        "    x = torch.permute(x, (0,2,1))\n",
        "    x = torch.squeeze(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "nQigV27-lNYN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spatial Graph Attention Convolution (Spatial-GACN) Layer\n",
        "\n",
        "\n",
        "*   Graph Attention Network (GAT)\n",
        "*   Graph Concolution Network (GCN)\n",
        "\n"
      ],
      "metadata": {
        "id": "lcAxejtNb5wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self, in_features = 200, out_features =200, dropout=0.3):\n",
        "    super(GAT, self).__init__()\n",
        "    self.dropout = dropout\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "    nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "    self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "    nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "    self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "  \n",
        "  def forward(self, h, adj):\n",
        "    # adj is the adjacency matrix\n",
        "\n",
        "    Wh = torch.matmul(h, self.W)\n",
        "    e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "    zero_vec = -9e15*torch.ones_like(e)\n",
        "    attention = torch.where(adj > 0, e, zero_vec)\n",
        "    attention = adj\n",
        "    attention = F.softmax(attention, dim=1)\n",
        "    attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "    h_prime = torch.matmul(attention.float(), Wh)\n",
        "\n",
        "    return F.elu(h_prime)\n",
        "  \n",
        "  def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        \n",
        "    Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "    Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "    \n",
        "    # broadcast add\n",
        "    e = Wh1 + Wh2.T\n",
        "    return self.leakyrelu(e)\n"
      ],
      "metadata": {
        "id": "P0g15CpjlLJx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FilterLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
        "        '''\n",
        "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
        "        '''\n",
        "        super(FilterLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        \n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        self.filter_square_matrix = None\n",
        "        if use_gpu:\n",
        "            self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
        "        else:\n",
        "            self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
        "        \n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "#         print(self.weight.data)\n",
        "#         print(self.bias.data)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.filter_square_matrix.matmul(self.weight.double()), self.bias)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'in_features=' + str(self.in_features) \\\n",
        "            + ', out_features=' + str(self.out_features) \\\n",
        "            + ', bias=' + str(self.bias is not None) + ')'"
      ],
      "metadata": {
        "id": "2swqf0WplJKP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "      def __init__(self, A, BNP, feature_size, Clamp_A = True, output_last = True):\n",
        "         super(GCN, self).__init__()\n",
        "         self.feature_size = feature_size\n",
        "         self.A = A\n",
        "         self.BNP = BNP\n",
        "         self.linear = FilterLinear(410, 410, self.A, bias= False)\n",
        "\n",
        "      def forward(self, input):\n",
        "      #some squeeze and unsqueeze\n",
        "        \n",
        "          x = torch.mul(self.A, torch.Tensor(self.BNP).double())\n",
        "          x = self.linear(x)\n",
        "          x = torch.matmul(x, input.double())\n",
        "          return x"
      ],
      "metadata": {
        "id": "JbNfIVsGfs08"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TSM Modules\n",
        "\n",
        "```TSM_first``` for the first TSM module and ```TSM_others``` for the remaining modules."
      ],
      "metadata": {
        "id": "QjCrllAUfVko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "class TSM_first(nn.Module):\n",
        "  def __init__(self,A, in_features= 200, out_features=200 ):\n",
        "    super(TSM_first,self).__init__()\n",
        "    self.GDCN1 = GDCN()\n",
        "    self.GDCN2 = GDCN()\n",
        "    self.gat = GAT(in_features= in_features, out_features= out_features)\n",
        "    self.A = A\n",
        "    self.GCN1 = GCN(A, BNP, in_features)\n",
        "    self.GCN2 = GCN(A, BNP, in_features)\n",
        "    self.BN1 = nn.BatchNorm1d(in_features)\n",
        "    self.BN2 = nn.BatchNorm1d(in_features)\n",
        "\n",
        "    self.nodevec1 = nn.Parameter(torch.randn(int(A.shape[0]), 10).to(device), requires_grad=True).to(device)\n",
        "    self.nodevec2 = nn.Parameter(torch.randn(10, int(A.shape[0])).to(device), requires_grad=True).to(device)\n",
        "\n",
        "    self.nodevec3 = nn.Parameter(torch.randn(int(A.shape[0]), 10).to(device), requires_grad=True).to(device)\n",
        "    self.nodevec4 = nn.Parameter(torch.randn(10, int(A.shape[0])).to(device), requires_grad=True).to(device)\n",
        "\n",
        "  def forward(self,input):\n",
        "    x = input.reshape((input.shape[0]*input.shape[1]), input.shape[2])\n",
        "    x1 = self.GDCN1(x)\n",
        "    x_middle = self.gat(x1,self.A)\n",
        "    x_middle = self.GCN1(x_middle)\n",
        "    x = self.BN1(x.permute(1,0)+x_middle.float()).permute(1,0)\n",
        "\n",
        "    x2 = self.GDCN2(x)\n",
        "    x_middle = self.gat(x2,self.A)\n",
        "    x_middle = self.GCN2(x_middle)\n",
        "    x = self.BN2(x.permute(1,0)+x_middle.float()).permute(1,0)\n",
        "\n",
        "    return x,x1,x2"
      ],
      "metadata": {
        "id": "E9GPeIW6lGvK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TSM_others(nn.Module):\n",
        "  def __init__(self,A, in_features= 200, out_features=200 ):\n",
        "    super(TSM_others,self).__init__()\n",
        "    self.GDCN1 = GDCN()\n",
        "    self.GDCN2 = GDCN()\n",
        "    self.gat = GAT(in_features= in_features, out_features=out_features )\n",
        "    self.A = A\n",
        "    self.GCN1 = GCN(A, BNP, in_features)\n",
        "    self.GCN2 = GCN(A, BNP, in_features)\n",
        "    self.BN1 = nn.BatchNorm1d(in_features)\n",
        "    self.BN2 = nn.BatchNorm1d(in_features)\n",
        "\n",
        "    self.nodevec1 = nn.Parameter(torch.randn(int(A.shape[0]), 10).to(device), requires_grad=True).to(device)\n",
        "    self.nodevec2 = nn.Parameter(torch.randn(10, int(A.shape[0])).to(device), requires_grad=True).to(device)\n",
        "\n",
        "    self.nodevec3 = nn.Parameter(torch.randn(int(A.shape[0]), 10).to(device), requires_grad=True).to(device)\n",
        "    self.nodevec4 = nn.Parameter(torch.randn(10, int(A.shape[0])).to(device), requires_grad=True).to(device)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x1 = self.GDCN1(x)\n",
        "    x_middle = self.gat(x1,self.A)\n",
        "    x_middle = self.GCN1(x_middle)\n",
        "    x = self.BN1(x.permute(1,0)+x_middle.float()).permute(1,0)\n",
        "\n",
        "    x2 = self.GDCN2(x)\n",
        "    x_middle = self.gat(x2,self.A)\n",
        "    x_middle = self.GCN2(x_middle)\n",
        "    x = self.BN2(x.permute(1,0)+x_middle.float()).permute(1,0)\n",
        "\n",
        "    return x,x1,x2"
      ],
      "metadata": {
        "id": "vsNs8N6jlDu9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class linear(nn.Module):\n",
        "    def __init__(self,c_in,c_out):\n",
        "        super(linear,self).__init__()\n",
        "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(1, 1), padding=(0,0), stride=(1,1), bias=True)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.mlp(x)\n"
      ],
      "metadata": {
        "id": "g3Vz8K9usQ_1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A component contains the 4 TSM modules, for each type of data"
      ],
      "metadata": {
        "id": "dhPQ7A7xkMZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class COMPONENT(nn.Module):\n",
        "  def __init__(self,A, inp = 200, out=200):\n",
        "    super(COMPONENT,self).__init__()\n",
        "    self.A = A\n",
        "    self.TSM1 = TSM_first(A= torch.tensor(self.A),in_features= inp,out_features=out)    \n",
        "    self.TSM2 = TSM_others(A= torch.tensor(self.A), in_features= inp,out_features=out)    \n",
        "    self.TSM3 = TSM_others(A= torch.tensor(self.A),in_features= inp,out_features=out)    \n",
        "    self.TSM4 = TSM_others(A= torch.tensor(self.A),in_features= inp,out_features=out)\n",
        "    self.RL = nn.ReLU()\n",
        "    self.conv1 = linear(1,1)\n",
        "    self.conv2 = linear(1,1)\n",
        "    self.linear = nn.Linear(inp, out_features=20)\n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "    x, x11,x12 = self.TSM1(x)\n",
        "    x, x21,x22 = self.TSM2(x)\n",
        "    x, x31,x32 = self.TSM3(x)\n",
        "    x, x41,x42 = self.TSM4(x)\n",
        "\n",
        "    x = self.RL(x11+x12+x21+x22+x31+x32+x41+x42)\n",
        "    x = torch.unsqueeze(torch.unsqueeze(x,0),0)\n",
        "    x = self.conv1(x)\n",
        "    x = self.RL(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.RL(x)\n",
        "    x = torch.squeeze(x)\n",
        "    x = self.linear(x)\n",
        "    return x.permute(1,0)"
      ],
      "metadata": {
        "id": "pCcwr8e6lA3C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main model"
      ],
      "metadata": {
        "id": "3cDcunJKlbMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_STGAC(nn.Module):\n",
        "  def __init__(self, A): \n",
        "    super(Multi_STGAC,self).__init__()\n",
        "    self.A=A\n",
        "    self.Recent_component = COMPONENT(A,20*10,20*10) # 20 is the batch size and 10 is the sequence lenght\n",
        "    self.Daily_component = COMPONENT(A,20*4,20*4)\n",
        "    self.Weekly_component = COMPONENT(A,20*2,20*2)\n",
        "    self.conv1 = nn.Conv2d(in_channels=3*410,\n",
        "                                    out_channels=410,\n",
        "                                    kernel_size=1, padding='same')\n",
        "    self.conv2 = nn.Conv2d(in_channels=410,\n",
        "                                    out_channels=410,\n",
        "                                    kernel_size=(1,1), padding='same')\n",
        "    self.elu = nn.ELU()\n",
        "  def forward(self,Xr,Xd,Xw):\n",
        "    X1 = self.Recent_component(Xr)\n",
        "    X2 = self.Daily_component(Xd)\n",
        "    X3 = self.Weekly_component(Xw)\n",
        "    #Concatenation operation\n",
        "    Y = torch.cat((X1,X2,X3),1)\n",
        "    Y = torch.unsqueeze(torch.unsqueeze(Y,2),3)\n",
        "    Y = self.conv1(Y)\n",
        "    Y = self.elu(Y)\n",
        "    Y = self.conv2(Y)\n",
        "    Y = torch.squeeze(Y)\n",
        "    return Y"
      ],
      "metadata": {
        "id": "aAw2q15rYzn2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute data preparation"
      ],
      "metadata": {
        "id": "WIuM-eVTSo0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader, max_load = PrepareDataset(main_df)"
      ],
      "metadata": {
        "id": "wUQZa3kOlumP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model function"
      ],
      "metadata": {
        "id": "mbepgjAgQyBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TrainModel(model, train_dataloader, valid_dataloader, learning_rate = 1e-5, num_epochs = 300, patience = 10, min_delta = 0.00001):\n",
        "    \n",
        "    \n",
        "    loss_MSE = torch.nn.MSELoss()\n",
        "    loss_L1 = torch.nn.L1Loss()\n",
        "\n",
        "    learning_rate = 1e-5\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "        \n",
        "    interval = 100\n",
        "    losses_train = []\n",
        "    losses_valid = []\n",
        "    losses_epochs_train = []\n",
        "    losses_epochs_valid = []\n",
        "    \n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "    \n",
        "    # Variables for Early Stopping\n",
        "    is_best_model = 0\n",
        "    patient_epoch = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "     \n",
        "        trained_number = 0\n",
        "        \n",
        "        valid_dataloader_iter = iter(valid_dataloader)\n",
        "        \n",
        "        losses_epoch_train = []\n",
        "        losses_epoch_valid = []\n",
        "        for data in train_dataloader:\n",
        "            \n",
        "            inputs_R, labels_R = data[0]\n",
        "            inputs_D, labels_D = data[1]\n",
        "            inputs_W, labels_W = data[2]\n",
        "\n",
        "            inputs_R, labels_R = Variable(inputs_R), Variable(labels_R)\n",
        "            inputs_D, labels_D = Variable(inputs_D), Variable(labels_D)\n",
        "            inputs_W, labels_W = Variable(inputs_W), Variable(labels_W)\n",
        "                \n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(inputs_R,inputs_D,inputs_W)\n",
        "            \n",
        "            loss_train = loss_MSE(outputs, torch.squeeze(labels_R))\n",
        "            \n",
        "            losses_train.append(loss_train.data)\n",
        "            losses_epoch_train.append(loss_train.data)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss_train.backward()\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            # validation \n",
        "            try: \n",
        "                data_val = next(valid_dataloader_iter)\n",
        "                inputs_val_R, labels_val_R = data_val[0]\n",
        "                inputs_val_D, labels_val_D = data_val[1]\n",
        "                inputs_val_W, labels_val_W = data_val[2]\n",
        "\n",
        "            except StopIteration:\n",
        "                valid_dataloader_iter = iter(valid_dataloader)\n",
        "                data_val = next(valid_dataloader_iter)\n",
        "                \n",
        "            \n",
        "            inputs_val_R, labels_val_R = Variable(inputs_val_R), Variable(labels_val_R)\n",
        "            inputs_val_D, labels_val_D = Variable(inputs_val_D), Variable(labels_val_D)\n",
        "            inputs_val_W, labels_val_W = Variable(inputs_val_W), Variable(labels_val_W)\n",
        "\n",
        "            outputs_val= model(inputs_val_R,inputs_val_D,inputs_val_W)\n",
        "\n",
        "            loss_valid = loss_MSE(outputs_val, torch.squeeze(labels_val_R))\n",
        "            losses_valid.append(loss_valid.data)\n",
        "            losses_epoch_valid.append(loss_valid.data)\n",
        "            \n",
        "            # output\n",
        "            trained_number += 1\n",
        "        \n",
        "        avg_losses_epoch_train = sum(losses_epoch_train) / float(len(losses_epoch_train))\n",
        "        avg_losses_epoch_valid = sum(losses_epoch_valid) / float(len(losses_epoch_valid))\n",
        "        losses_epochs_train.append(avg_losses_epoch_train)\n",
        "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
        "        \n",
        "        # Early Stopping\n",
        "        if epoch == 0:\n",
        "            is_best_model = 1\n",
        "            best_model = model\n",
        "            min_loss_epoch_valid = 10000.0\n",
        "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
        "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
        "        else:\n",
        "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
        "                is_best_model = 1\n",
        "                best_model = model\n",
        "                min_loss_epoch_valid = avg_losses_epoch_valid \n",
        "                patient_epoch = 0\n",
        "            else:\n",
        "                is_best_model = 0\n",
        "                patient_epoch += 1\n",
        "                if patient_epoch >= patience:\n",
        "                    print('Early Stopped at Epoch:', epoch)\n",
        "                    break\n",
        "        \n",
        "        # Print training parameters\n",
        "        cur_time = time.time()\n",
        "        print('Epoch: {}, train_loss: {}, val_loss:{} ,time: {}, best model: {}'.format( \\\n",
        "                    epoch, \\\n",
        "                    np.around(avg_losses_epoch_train, decimals=8),\\\n",
        "                    np.around(avg_losses_epoch_valid, decimals=8),\\\n",
        "                    np.around([cur_time - pre_time] , decimals=2),\\\n",
        "                    is_best_model) )\n",
        "        pre_time = cur_time\n",
        "    \n",
        "    return model, [losses_train, losses_epochs_train, losses_epoch_valid], is_best_model"
      ],
      "metadata": {
        "id": "w_r8LpbhmRgO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute and test model"
      ],
      "metadata": {
        "id": "7bblF4JZQ3R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Multi_STGAC(torch.tensor(A))"
      ],
      "metadata": {
        "id": "sZudzI6TymcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adba2890-8e92-4d05-dddb-4786f9fcca64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, [losses_train, losses_epochs_train, losses_epoch_valid], is_best_model = TrainModel(model, train_loader, valid_loader)"
      ],
      "metadata": {
        "id": "P82BMBZtoAaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmse(input,target):\n",
        "  #input and target of shape (x,y)\n",
        "  N = target.shape[1]\n",
        "  return np.sqrt(((input - target)**2).sum(axis=1)/N)\n",
        "\n",
        "def mae(input,target):\n",
        "  #input and target of shape (x,y)\n",
        "  N = target.shape[1]\n",
        "  return np.abs(input - target).sum(axis=1)/N\n",
        "\n",
        "# for the 4-bin accuracy\n",
        "def ranging(table):\n",
        "  return 1*(table>=0)*(table<=5) + 2*(table>=6)*(table<=10) + 3*(table>=11)*(table<=15)+ 4*(table>=16)\n",
        "\n",
        "def bins_accuracy(output,label):\n",
        "  output = ranging(output)\n",
        "  label = ranging(label)\n",
        "  return accuracy_score(output,label)"
      ],
      "metadata": {
        "id": "Y0lnWwl6ztm_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TestModel(model, test_dataloader, max_load):\n",
        "    \n",
        "    cur_time = time.time()\n",
        "    pre_time = time.time()\n",
        "    \n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    \n",
        "    \n",
        "    tested_batch = 0\n",
        "    \n",
        "    losses_rmse = []\n",
        "    losses_mae = []\n",
        "    acc = []\n",
        "    acc_bin = []\n",
        "    \n",
        "    \n",
        "    for data in test_dataloader:\n",
        "        inputs_R, labels_R = data[0]\n",
        "        inputs_D, labels_D = data[1]\n",
        "        inputs_W, labels_W = data[2]\n",
        "\n",
        "        inputs_R, labels_R = Variable(inputs_R), Variable(labels_R)\n",
        "        inputs_D, labels_D = Variable(inputs_D), Variable(labels_D)\n",
        "        inputs_W, labels_W = Variable(inputs_W), Variable(labels_W)\n",
        "\n",
        "        \n",
        "        outputs =  model(inputs_R,inputs_D,inputs_W)\n",
        "    \n",
        "    \n",
        "        loss_mae = mae(outputs.detach().numpy()*max_load, torch.squeeze(labels_R).detach().numpy()*max_load)\n",
        "        losses_mae.append(loss_mae)\n",
        "\n",
        "        loss_rmse = rmse(outputs.detach().numpy()*max_load,torch.squeeze(labels_R).detach().numpy()*max_load)\n",
        "        losses_rmse.append(loss_rmse)\n",
        "\n",
        "        accuracy = accuracy_score(np.rint(outputs.detach().numpy().flatten()*max_load),np.rint(torch.squeeze(labels_R).detach().numpy().flatten()*max_load))\n",
        "        acc.append(accuracy)\n",
        "\n",
        "        accuracy_bins = bins_accuracy(np.rint(outputs.detach().numpy().flatten()*max_load),np.rint(torch.squeeze(labels_R).detach().numpy().flatten()*max_load))\n",
        "        acc_bin.append(accuracy_bins)\n",
        "\n",
        "    mean_mae = np.mean(losses_mae)\n",
        "    mean_rmse = np.mean(losses_rmse)\n",
        "    mean_acc = np.mean(acc)\n",
        "    mean_acc_bin = np.mean(acc_bin)\n",
        "\n",
        "    print('Tested: mae_mean: {}'.format(mean_mae))\n",
        "    print('Tested: rmse_mean: {}'.format(mean_rmse))\n",
        "    print('Tested: accuracy_mean: {}'.format(mean_acc))\n",
        "    print('Tested: accuracy_bin_mean: {}'.format(mean_acc_bin))\n",
        "    return  None"
      ],
      "metadata": {
        "id": "TgliCVP6tevb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestModel(model, test_loader,max_load)"
      ],
      "metadata": {
        "id": "of4w_pCfPweH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}